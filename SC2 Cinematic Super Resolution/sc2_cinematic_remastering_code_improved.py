# -*- coding: utf-8 -*-
"""SC2 Cinematic remastering Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvL9QJVD52udVHz0WIQdEh93OZFBigC6
"""

from google.colab import drive
drive.mount('/content/gdrive/')

import warnings
warnings.filterwarnings(action='ignore')

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf
import shutil

from PIL import Image
from keras.preprocessing.image import img_to_array

from tensorflow import keras
from keras.layers import Conv2D, Input, Activation
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from keras import regularizers
from keras.preprocessing.image import ImageDataGenerator
from PIL import Image
from keras.preprocessing.image import img_to_array
from keras.optimizers import Adam
from keras import backend as K
from tensorflow.python.keras.layers import Add, Conv2D, Input, Lambda

def upsample(x):
    x = tf.concat([x, x, x, x], axis=-1)
    x = tf.nn.depth_to_space(x, 2)

    return x

def psnr_loss(a,b):
   x = tf.image.psnr(a, b, 1, name=None)
   return x

def ssim(y_true, y_pred):
    return K.expand_dims(tf.image.ssim(y_true, y_pred, 255.), 0)

x_train_dir = '/content/x_train/'
y_train_dir = '/content/y_train/'
x_test_dir = '/content/x_test/'
y_test_dir = '/content/y_test/'
os.mkdir(x_train_dir)
os.mkdir(y_train_dir)
os.mkdir(x_test_dir)
os.mkdir(y_test_dir)

shutil.rmtree(x_train_dir)
shutil.rmtree(y_train_dir)
shutil.rmtree(x_test_dir)
shutil.rmtree(y_test_dir)
# os.rmdir(x_train_dir)
# os.rmdir(y_train_dir)
# os.rmdir(x_test_dir)
# os.rmdir(y_test_dir)

vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Dataset/SC2 Super Resolution/720p_30fps.mp4') 
count = 0 
while(vidcap.isOpened()):
   ret, image = vidcap.read()
   # 30프레임당 하나씩 이미지 추출 
   if(int(vidcap.get(1)) % 20 == 0 and 150 < count < 3500):  ## 이걸 만들어 둔 이유가 램부족떄문입니다!
     cv2.imwrite("/content/x_train/%d.jpg" % count, image)
   elif(int(vidcap.get(1)) % 25 == 0 and count > 3500): 
     cv2.imwrite("/content/x_test/%d.jpg" % count, image)
  #  print('Saved frame%d.jpg' % count) 
   count += 1 
   if count == 4944:
     break
vidcap.release()

vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Dataset/SC2 Super Resolution/wotl_544p.mp4') 
count = 0
while(vidcap.isOpened()):
   ret, image = vidcap.read() 
   # 30프레임당 하나씩 이미지 추출 
   if(int(vidcap.get(1)) % 18 == 0 and 150 < count < 5200): ## 이걸 만들어 둔 이유가 램부족떄문입니다!
     cv2.imwrite("/content/y_train/%d.jpg" % count, image)
   elif(int(vidcap.get(1)) % 18 == 0 and 5200 < count < 5900): 
     cv2.imwrite("/content/y_test/%d.jpg" % count, image)
  #  print('Saved frame%d.jpg' % count) 2223
   if count >= 5900:
     break
   count += 1 
vidcap.release()

count

name_x_train = sorted(os.listdir('/content/x_train'))
name_y_train = sorted(os.listdir('/content/y_train'))
name_x_test = sorted(os.listdir('/content/x_test'))
name_y_test = sorted(os.listdir('/content/y_test'))

"""# Data Augmentation
blur는 수렴이 안됨. 좋을 것 같았지만 폐기
flip 도전
"""

x_train = []

for i in name_y_train:
  img1 = cv2.cvtColor(cv2.imread('/content/y_train/' + i), cv2.COLOR_BGR2RGB)
  img1 = cv2.pyrDown(img1)
  x_train.append(img_to_array(Image.fromarray(img1)))
num = len(x_train)
for i in range(num):
  x_train.append(cv2.flip(x_train[i], 1))
for i in range(num):
  x_train.append(cv2.flip(x_train[i], 2))

x_train = np.array(x_train)
x_train = x_train / 255.0

y_train = []

for i in name_y_train:
  img = cv2.cvtColor(cv2.imread('/content/y_train/' + i), cv2.COLOR_BGR2RGB)
  y_train.append(img_to_array(Image.fromarray(img)))
num = len(y_train)
for i in range(num):
  y_train.append(cv2.flip(y_train[i], 1))
for i in range(num):
  y_train.append(cv2.flip(y_train[i], 2))

y_train = np.array(y_train)
y_train = y_train / 255.0

x_test = []

for i in name_y_test:
    img = cv2.cvtColor(cv2.imread('/content/y_test/' + i), cv2.COLOR_BGR2RGB)
    img = cv2.pyrDown(img)
    # img = cv2.resize(img, (512,288), interpolation = cv2.INTER_CUBIC)
    x_test.append(img_to_array(Image.fromarray(img)))

x_test = np.array(x_test)
x_test = x_test / 255.0

y_test = []

for i in name_y_test:
    img = cv2.cvtColor(cv2.imread('/content/y_test/' + i), cv2.COLOR_BGR2RGB)
    # img = cv2.resize(img, (512*2,288*2), interpolation = cv2.INTER_CUBIC)
    y_test.append(img_to_array(Image.fromarray(img)))

y_test = np.array(y_test)
y_test = y_test / 255.0

x_test_resized = []
for i in name_y_test:
  subset = cv2.imread('/content/y_test/' + i)
  subset = cv2.pyrDown(subset)
  subset = cv2.resize(subset, (1280, 544), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.resize(subset, (512,288), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.resize(subset, (1280, 720), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.pyrUp(subset)
  img1 = cv2.cvtColor(subset, cv2.COLOR_BGR2RGB)
  x_test_resized.append(img_to_array(Image.fromarray(img1)))

x_test_resized = np.array(x_test_resized)
x_test_resized = x_test_resized / 255.0

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
print(x_test_resized.shape)

"""# Model 1
EDSR
"""

def residual_block(blockInput, num_filters = 64):
    x = tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(10e-10))(blockInput)
    x = tf.keras.layers.Conv2D(num_filters, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(10e-10))(x)
    x = tf.keras.layers.Add()([x, blockInput])
    return x

inputs = keras.Input(shape=(None, None, 3))
n_filters = 64

x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(10e-10))(inputs)
for i in range(20 - 1):
  x = residual_block(x, num_filters = n_filters)

x = upsample(x)
outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding="same",  activity_regularizer=regularizers.l1(10e-10))(x)

model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay = 1e-5)
model.compile(optimizer=optimizer, loss='mae', metrics=[psnr_loss])

model.summary()

callbacks_list = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath = 'models/EDSR_model_free_size.h5',
        monitor='val_loss',
        save_best_only=True
    )]

# learning_rate=3e-3, decay = 3e-6 수렴 실패
# learning_rate=1e-3, decay = 1e-5
model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs = 200, callbacks=callbacks_list)

# learning_rate=5e-4, decay=0.9

model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs=150, callbacks=callbacks_list)

"""# Model 2
RDN
"""

def upsample(x):
    x = tf.concat([x, x, x, x], axis=-1)
    x = tf.nn.depth_to_space(x, 2)

    return x

def RDB(input, num_filters = 64, l1_alpha = 10e-10):
  con1 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(input)
  con1 = tf.keras.layers.Add()([con1, input])
  con2 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con1)
  con2 = tf.keras.layers.Add()([con1, con2, input])
  con3 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con2)
  con3 = tf.keras.layers.Add()([con1, con2, con3, input])
  concat = tf.keras.layers.Concatenate()([input, con1, con2, con3])
  con4 = Conv2D(num_filters, (1,1), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(concat)
  con4 = tf.keras.layers.Add()([con4, input])
  return con4

def RDN(input_shape = (None, None, 3)):
    def upsample(x):
        x = tf.concat([x, x, x, x], axis=-1)
        x = tf.nn.depth_to_space(x, 2)

        return x
    def RDB(input, num_filters = 64, l1_alpha = 10e-10):
        con1 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(input)
        con1 = tf.keras.layers.Add()([con1, input])
        con2 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con1)
        con2 = tf.keras.layers.Add()([con1, con2, input])
        con3 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con2)
        con3 = tf.keras.layers.Add()([con1, con2, con3, input])
        concat = tf.keras.layers.Concatenate()([input, con1, con2, con3])
        con4 = Conv2D(num_filters, (1,1), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(concat)
        con4 = tf.keras.layers.Add()([con4, input])
        return con4

    inputs = keras.Input(shape = input_shape)
    n_filters = 64
    l1_alpha = 10e-10

    global_average = tf.keras.layers.Conv2D(n_filters/2, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(inputs)
    x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(global_average)

    # RDN n_block = 5
    Dense_block_1 = RDB(x)
    Dense_block_2 = RDB(Dense_block_1)
    Dense_block_3 = RDB(Dense_block_2)
    Dense_block_4 = RDB(Dense_block_3)
    Dense_block_5 = RDB(Dense_block_4)

    concat_total = tf.keras.layers.Concatenate()([Dense_block_1, Dense_block_2, Dense_block_3, Dense_block_4, Dense_block_5])



    # RDN n_block = 10
    # Dense_block_1 = RDB(x)
    # Dense_block_2 = RDB(Dense_block_1)
    # Dense_block_3 = RDB(Dense_block_2)
    # Dense_block_4 = RDB(Dense_block_3)
    # Dense_block_5 = RDB(Dense_block_4)
    # Dense_block_6 = RDB(Dense_block_5)
    # Dense_block_7 = RDB(Dense_block_6)
    # Dense_block_8 = RDB(Dense_block_7)
    # Dense_block_9 = RDB(Dense_block_8)
    # Dense_block_10 = RDB(Dense_block_9)

    # concat_total = tf.keras.layers.Concatenate()([Dense_block_1, Dense_block_2, Dense_block_3, Dense_block_4, Dense_block_5,
    #                                               Dense_block_6, Dense_block_7, Dense_block_8, Dense_block_9, Dense_block_10])

    # RDN n_block = 20

    # Dense_block_1 = RDB(x)
    # Dense_block_2 = RDB(Dense_block_1)
    # Dense_block_3 = RDB(Dense_block_2)
    # Dense_block_4 = RDB(Dense_block_3)
    # Dense_block_5 = RDB(Dense_block_4)
    # Dense_block_6 = RDB(Dense_block_5)
    # Dense_block_7 = RDB(Dense_block_6)
    # Dense_block_8 = RDB(Dense_block_7)
    # Dense_block_9 = RDB(Dense_block_8)
    # Dense_block_10 = RDB(Dense_block_9)
    # Dense_block_11 = RDB(Dense_block_10)
    # Dense_block_12 = RDB(Dense_block_11)
    # Dense_block_13 = RDB(Dense_block_12)
    # Dense_block_14 = RDB(Dense_block_13)
    # Dense_block_15 = RDB(Dense_block_14)
    # Dense_block_16 = RDB(Dense_block_15)
    # Dense_block_17 = RDB(Dense_block_16)
    # Dense_block_18 = RDB(Dense_block_17)
    # Dense_block_19 = RDB(Dense_block_18)
    # Dense_block_20 = RDB(Dense_block_19)

    # concat_total = tf.keras.layers.Concatenate()([Dense_block_1, Dense_block_2, Dense_block_3, Dense_block_4, Dense_block_5,
    #                                               Dense_block_6, Dense_block_7, Dense_block_8, Dense_block_9, Dense_block_10,
    #                                               Dense_block_11, Dense_block_12, Dense_block_13, Dense_block_14, Dense_block_15,
    #                                               Dense_block_16, Dense_block_17, Dense_block_18, Dense_block_19, Dense_block_20
    #                                               ])
    conv_1 = Conv2D(n_filters, (1, 1), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(concat_total)
    conv_total = Conv2D(n_filters, (5, 5), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(conv_1)

    upsample = upsample(conv_total)
    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(upsample)

    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    return model
    # optimizer = tf.keras.optimizers.Adam(learning_rate=15e-5, decay = 1e-5)
    # model.compile(optimizer=optimizer, loss='mae', metrics=[psnr_loss])

    # model.summary()

model_rdn = RDN()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
model_rdn.compile(optimizer=optimizer, loss = tf.keras.losses.MeanSquaredError(), metrics=[psnr_loss])

callbacks_list = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath = 'models/RDN_model_1e-4_learning_free_size_자유의_날개.h5',
        monitor = 'val_psnr_loss',
        save_best_only=True
    )]

history = model_rdn.fit(x_train, y_train, validation_data = (x_test, y_test), batch_siz3=4,
          epochs = 300, callbacks=callbacks_list)
# Epoch 254/300
# 251/251 [==============================] - 200s 798ms/step - loss: 0.0077 - psnr_loss: 37.1783 - val_loss: 0.0123 - val_psnr_loss: 32.7666





"""# Model Prediction"""

dependencies = {'psnr_loss': psnr_loss}
model = tf.keras.models.load_model('/content/models/RDN_model_12e-5learning_free_size_자유의_날개.h5', custom_objects = {'psnr_loss': psnr_loss})

x_test_resized.shape

predict = model.predict(x_test, batch_size = 4)

predict_y = model.predict(y_test, batch_size = 4)

predictx4 = model.predict(predict, batch_size = 2)

print(x_test_resized.shape)
print(predict.shape)



for i in range(10):
  plt.figure(figsize=(100, 70))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predict[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

for i in range(len(predict)):
  plt.figure(figsize=(100, 70))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predictx4[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

# 처음 3초, 뒤에 10초 제거

for i in range(10):
  plt.figure(figsize=(100, 50))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predict_y[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

import math
def rmse_score(true, pred):
    score = math.sqrt(np.mean((true-pred)**2))
    return score

def psnr_score(true, pred, pixel_max):
    score = 20*np.log10(pixel_max/rmse_score(true, pred))
    return score

"""# SRResnet fixed size PSNR"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# SRresnet free size PSNR"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# SRresnet free size PSNR + blurring"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR MSE loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR ssim loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR psnr loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)
