# -*- coding: utf-8 -*-
"""SC2 Cinematic remastering Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvL9QJVD52udVHz0WIQdEh93OZFBigC6
"""

from google.colab import drive
drive.mount('/content/gdrive/')

import warnings
warnings.filterwarnings(action='ignore')

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf
import shutil

from PIL import Image
from keras.preprocessing.image import img_to_array

from tensorflow import keras
from keras.layers import Conv2D, Input, Activation
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from keras import regularizers
from keras.preprocessing.image import ImageDataGenerator
from PIL import Image
from keras.preprocessing.image import img_to_array
from keras.optimizers import Adam
from keras import backend as K
from tensorflow.python.keras.layers import Add, Conv2D, Input, Lambda

def upsample(x):
    x = tf.concat([x, x, x, x], axis=-1)
    x = tf.nn.depth_to_space(x, 2)

    return x

def psnr_loss(a,b):
   x = tf.image.psnr(a, b, 1, name=None)
   return x

def ssim(y_true, y_pred):
    return K.expand_dims(tf.image.ssim(y_true, y_pred, 255.), 0)

x_train_dir = '/content/x_train/'
y_train_dir = '/content/y_train/'
x_test_dir = '/content/x_test/'
y_test_dir = '/content/y_test/'
os.mkdir(x_train_dir)
os.mkdir(y_train_dir)
os.mkdir(x_test_dir)
os.mkdir(y_test_dir)

shutil.rmtree(x_train_dir)
shutil.rmtree(y_train_dir)
shutil.rmtree(x_test_dir)
shutil.rmtree(y_test_dir)
# os.rmdir(x_train_dir)
# os.rmdir(y_train_dir)
# os.rmdir(x_test_dir)
# os.rmdir(y_test_dir)

vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Dataset/SC2 Super Resolution/720p_30fps.mp4') 
count = 0 
while(vidcap.isOpened()):
   ret, image = vidcap.read()
   # 30프레임당 하나씩 이미지 추출 
   if(int(vidcap.get(1)) % 24 == 0):  ## 이걸 만들어 둔 이유가 램부족떄문입니다!
     cv2.imwrite("/content/x_train/%d.jpg" % count, image)
   elif(int(vidcap.get(1)) % 247 == 0): 
     cv2.imwrite("/content/x_test/%d.jpg" % count, image)
  #  print('Saved frame%d.jpg' % count) 
   count += 1 
   if count == 4944:
     break
vidcap.release()

vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Dataset/SC2 Super Resolution/wotl_544p.mp4') 
count = 0
while(vidcap.isOpened()):
   ret, image = vidcap.read() 
   # 30프레임당 하나씩 이미지 추출 
   if(int(vidcap.get(1)) % 24 == 0): ## 이걸 만들어 둔 이유가 램부족떄문입니다!
     cv2.imwrite("/content/y_train/%d.jpg" % count, image)
   elif(int(vidcap.get(1)) % 247 == 0): 
     cv2.imwrite("/content/y_test/%d.jpg" % count, image)
  #  print('Saved frame%d.jpg' % count) 
   if count == 4944:
     break
   count += 1 
vidcap.release()

name_x_train = sorted(os.listdir('/content/x_train'))
name_y_train = sorted(os.listdir('/content/y_train'))
name_x_test = sorted(os.listdir('/content/x_test'))
name_y_test = sorted(os.listdir('/content/y_test'))

"""# Data Augmentation
blur는 수렴이 안됨. 좋을 것 같았지만 폐기
flip 도전
"""

x_train = []

for i in name_x_train:
  img1 = cv2.cvtColor(cv2.imread('/content/x_train/' + i), cv2.COLOR_BGR2RGB)
  img1 = cv2.pyrDown(img1)
  # img1 = cv2.resize(img1, (512,288), interpolation = cv2.INTER_CUBIC)
  x_train.append(img_to_array(Image.fromarray(img1)))
num = len(x_train)
for i in range(num):
  x_train.append(cv2.flip(x_train[i], 1))
for i in range(num):
  x_train.append(cv2.flip(x_train[i], 2))

x_train = np.array(x_train)
x_train = x_train / 255.0

y_train = []

for i in name_x_train:
  img = cv2.cvtColor(cv2.imread('/content/x_train/' + i), cv2.COLOR_BGR2RGB)
  # img = cv2.resize(img, (512*2,288*2), interpolation = cv2.INTER_CUBIC)
  y_train.append(img_to_array(Image.fromarray(img)))
num = len(y_train)
for i in range(num):
  y_train.append(cv2.flip(y_train[i], 1))
for i in range(num):
  y_train.append(cv2.flip(y_train[i], 2))

y_train = np.array(y_train)
y_train = y_train / 255.0

x_test = []

for i in name_y_test:
    img = cv2.cvtColor(cv2.imread('/content/y_test/' + i), cv2.COLOR_BGR2RGB)
    img = cv2.pyrDown(img)
    # img = cv2.resize(img, (512,288), interpolation = cv2.INTER_CUBIC)
    x_test.append(img_to_array(Image.fromarray(img)))

x_test = np.array(x_test)
x_test = x_test / 255.0

y_test = []

for i in name_y_test:
    img = cv2.cvtColor(cv2.imread('/content/y_test/' + i), cv2.COLOR_BGR2RGB)
    # img = cv2.resize(img, (512*2,288*2), interpolation = cv2.INTER_CUBIC)
    y_test.append(img_to_array(Image.fromarray(img)))

y_test = np.array(y_test)
y_test = y_test / 255.0

x_test_resized = []
for i in name_y_test:
  subset = cv2.imread('/content/y_test/' + i)
  subset = cv2.pyrDown(subset)
  subset = cv2.resize(subset, (1280,544), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.resize(subset, (512,288), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.resize(subset, (512*2,288*2), interpolation = cv2.INTER_CUBIC)
  # subset = cv2.pyrUp(subset)
  img1 = cv2.cvtColor(subset, cv2.COLOR_BGR2RGB)
  x_test_resized.append(img_to_array(Image.fromarray(img1)))

x_test_resized = np.array(x_test_resized)
x_test_resized = x_test_resized / 255.0

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
print(x_test_resized.shape)

"""# Model 1
EDSR
"""

def residual_block(blockInput, num_filters = 64):
    x = tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(10e-10))(blockInput)
    x = tf.keras.layers.Conv2D(num_filters, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(10e-10))(x)
    x = tf.keras.layers.Add()([x, blockInput])
    return x

inputs = keras.Input(shape=(None, None, 3))
n_filters = 64

x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(10e-10))(inputs)
for i in range(20 - 1):
  x = residual_block(x, num_filters = n_filters)

x = upsample(x)
outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding="same",  activity_regularizer=regularizers.l1(10e-10))(x)

model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay = 1e-5)
model.compile(optimizer=optimizer, loss='mae', metrics=[psnr_loss])

model.summary()

callbacks_list = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath = 'models/EDSR_model_free_size.h5',
        monitor='val_loss',
        save_best_only=True
    )]

# learning_rate=3e-3, decay = 3e-6 수렴 실패
# learning_rate=1e-3, decay = 1e-5
model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs = 200, callbacks=callbacks_list)

# learning_rate=5e-4, decay=0.9

model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs=150, callbacks=callbacks_list)

"""# Model 2
RDN
"""

def upsample(x):
    x = tf.concat([x, x, x, x], axis=-1)
    x = tf.nn.depth_to_space(x, 2)

    return x

def RDB(input, num_filters = 64, l1_alpha = 10e-10):
  con1 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(input)
  con1 = tf.keras.layers.Add()([con1, input])
  con2 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con1)
  con2 = tf.keras.layers.Add()([con1, con2, input])
  con3 = tf.keras.layers.Conv2D(num_filters, (3,3), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(con2)
  con3 = tf.keras.layers.Add()([con1, con2, con3, input])
  concat = tf.keras.layers.Concatenate()([input, con1, con2, con3])
  con4 = Conv2D(num_filters, (1,1), activation='relu', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(concat)
  con4 = tf.keras.layers.Add()([con4, input])
  return con4

inputs = keras.Input(shape=(None, None, 3))
n_filters = 64
l1_alpha = 10e-10

global_average = tf.keras.layers.Conv2D(n_filters/2, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(inputs)
x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(global_average)

# RDN n_block = 10
Dense_block_1 = RDB(x)
Dense_block_2 = RDB(Dense_block_1)
Dense_block_3 = RDB(Dense_block_2)
Dense_block_4 = RDB(Dense_block_3)
Dense_block_5 = RDB(Dense_block_4)
Dense_block_6 = RDB(Dense_block_5)
Dense_block_7 = RDB(Dense_block_6)
Dense_block_8 = RDB(Dense_block_7)
Dense_block_9 = RDB(Dense_block_8)
Dense_block_10 = RDB(Dense_block_9)

concat_total = tf.keras.layers.Concatenate()([Dense_block_1, Dense_block_2, Dense_block_3, Dense_block_4, Dense_block_5,
                                              Dense_block_6, Dense_block_7, Dense_block_8, Dense_block_9, Dense_block_10])

# RDN n_block = 20

# Dense_block_1 = RDB(x)
# Dense_block_2 = RDB(Dense_block_1)
# Dense_block_3 = RDB(Dense_block_2)
# Dense_block_4 = RDB(Dense_block_3)
# Dense_block_5 = RDB(Dense_block_4)
# Dense_block_6 = RDB(Dense_block_5)
# Dense_block_7 = RDB(Dense_block_6)
# Dense_block_8 = RDB(Dense_block_7)
# Dense_block_9 = RDB(Dense_block_8)
# Dense_block_10 = RDB(Dense_block_9)
# Dense_block_11 = RDB(Dense_block_10)
# Dense_block_12 = RDB(Dense_block_11)
# Dense_block_13 = RDB(Dense_block_12)
# Dense_block_14 = RDB(Dense_block_13)
# Dense_block_15 = RDB(Dense_block_14)
# Dense_block_16 = RDB(Dense_block_15)
# Dense_block_17 = RDB(Dense_block_16)
# Dense_block_18 = RDB(Dense_block_17)
# Dense_block_19 = RDB(Dense_block_18)
# Dense_block_20 = RDB(Dense_block_19)

# concat_total = tf.keras.layers.Concatenate()([Dense_block_1, Dense_block_2, Dense_block_3, Dense_block_4, Dense_block_5,
#                                               Dense_block_6, Dense_block_7, Dense_block_8, Dense_block_9, Dense_block_10,
#                                               Dense_block_11, Dense_block_12, Dense_block_13, Dense_block_14, Dense_block_15,
#                                               Dense_block_16, Dense_block_17, Dense_block_18, Dense_block_19, Dense_block_20
#                                               ])
conv_1 = Conv2D(n_filters, (1, 1), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(concat_total)
conv_total = Conv2D(n_filters, (5, 5), activation=None, padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(conv_1)

upsample = upsample(conv_total)
outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding="same",  activity_regularizer=regularizers.l1(l1_alpha))(upsample)

model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay = 1e-5)
model.compile(optimizer=optimizer, loss='mae', metrics=[psnr_loss])

model.summary()

callbacks_list = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath = 'models/RDN_model_free_size.h5',
        monitor='val_loss',
        save_best_only=True
    )]

model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs = 200, callbacks=callbacks_list)

"""# Model Prediction"""

# compile model
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['acc', km.binary_precision(), km.binary_recall()])

# load model
model = load_model(os.path.join(FLAGS.init_checkpoint,model_name), custom_objects={'binary_precision':km.binary_precision(), 'binary_recall':km.binary_recall()}

dependencies = {'psnr_loss': psnr_loss}
model = tf.keras.models.load_model('/content/models/EDSR_model_free_size.h5', custom_objects = {'psnr_loss': psnr_loss})

x_test_org_resized = []

for i in name_x_test:
    img = cv2.cvtColor(cv2.imread('/content/x_test/' + i), cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (1280,720), interpolation = cv2.INTER_CUBIC)
    x_test_org_resized.append(img_to_array(Image.fromarray(img)))

x_test_org_resized = np.array(x_test_org_resized)
x_test_org_resized = x_test_org_resized / 255.0

y_test_org = []

for i in name_y_test:
    img = cv2.cvtColor(cv2.imread('/content/y_test/' + i), cv2.COLOR_BGR2RGB)
    y_test_org.append(img_to_array(Image.fromarray(img)))

y_test_org = np.array(y_test_org)
y_test_org = y_test_org / 255.0

print(x_test_org.shape)
print(y_test_org.shape)

x_test_resized.shape

model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size=2,
          epochs=2000, callbacks=callbacks_list)

# predict = model.predict(x_test_resized)
predict = model.predict(x_test, batch_size = 4)
# predict = cv2.cvtColor(predict, cv2.COLOR_BGR2RGB

predict_2 = model.predict(predict, batch_size = 4)

x_test_resized.shape

print(x_test.shape)
print(predict.shape)

i = 1
plt.subplot(1, 3, 1)
plt.title('input')
plt.imshow(x_test_resized[i])
plt.subplot(1, 3, 2)
plt.title('output')
plt.imshow(predict[i])
plt.subplot(1, 3, 3)
plt.title('input')
plt.imshow(y_test[i])

for i in range(len(predict)):
  plt.figure(figsize=(100, 50))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predict_2[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

for i in range(len(predict)):
  plt.figure(figsize=(100, 50))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predict[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

predict_x4.shape

for i in range(len(predict)):
  plt.figure(figsize=(100, 50))
  plt.subplot(1, 3, 1)
  plt.title('input')
  plt.imshow(x_test_resized[i])
  plt.subplot(1, 3, 2)
  plt.title('output')
  plt.imshow(predict_x4[i])
  plt.subplot(1, 3, 3)
  plt.title('input')
  plt.imshow(y_test[i])

y_test.shape

import math
def rmse_score(true, pred):
    score = math.sqrt(np.mean((true-pred)**2))
    return score

def psnr_score(true, pred, pixel_max):
    score = 20*np.log10(pixel_max/rmse_score(true, pred))
    return score

"""# SRResnet fixed size PSNR"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# SRresnet free size PSNR"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# SRresnet free size PSNR + blurring"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR MSE loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR ssim loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(len(predict)):
  mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)

"""# EDSR psnr loss n_filter = 64, n_block = 20"""

mean_predict, mean_org = 0, 0
for i in range(4,len(predict)):
  print(i)
  # mean_org += psnr_score(x_test_resized[i], y_test[i],1)
  mean_predict += psnr_score(predict[i], y_test[i],1)
  print(i,' 번째:', psnr_score(predict[i], y_test[i],1) - psnr_score(x_test_resized[i], y_test[i],1))
  # print(i,' 번째 predict psnr:', psnr_score(x_test_resize[i], y_test[i],1))
mean_predict, mean_org = mean_predict / len(predict), mean_org / len(predict)
print('original mean psnr:', mean_org)
print('predict mean psnr:', mean_predict)